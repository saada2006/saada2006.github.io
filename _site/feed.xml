<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-02-14T22:09:13-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Saad’s blog</title><subtitle>Home</subtitle><author><name>Saad Amin</name></author><entry><title type="html">GPU BVH Traversal Algorithms</title><link href="http://localhost:4000/blogs/gpu-bvh-traversal-algorithms" rel="alternate" type="text/html" title="GPU BVH Traversal Algorithms" /><published>2022-07-22T00:00:00-07:00</published><updated>2022-07-22T00:00:00-07:00</updated><id>http://localhost:4000/blogs/gpu-bvh-traversal-algos</id><content type="html" xml:base="http://localhost:4000/blogs/gpu-bvh-traversal-algorithms">&lt;h1 id=&quot;a-quick-primer-on-how-gpus-crunch-numbers-and-stuff&quot;&gt;A Quick Primer on How GPUs Crunch Numbers and Stuff&lt;/h1&gt;

&lt;p&gt;GPUs are essentially just like the CPU except with one small difference: every thing is massively parrallelized. Work is executed in the single instruction multiple thread (SIMT) model, meaning that a group of threads (usually 32) exeucte an instruction altoghter at once. This group of threads is known as a &lt;em&gt;warp&lt;/em&gt; and on Nvidia GPUs, each warp executes on a single CUDA core, with the average GPU having thousands of them. The SIMT model allows your GPU to crunch a lot of numbers, for example, in a video game where millions of pixels need to undergo the same lighting calculations. However, an avid reader would realize the SIMT model would probably not work (well) at all with conditional statements like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;if&lt;/code&gt;. Suppose I have a block of code like:&lt;/p&gt;

&lt;div class=&quot;language-glsl highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cond&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;MyFuncA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;MyFuncB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the SIMT model, all threads where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cond&lt;/code&gt; is true will proceed on to execute &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MyFuncA()&lt;/code&gt; while the other threads wait and do nothing. One the threads that were executing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MyFuncA()&lt;/code&gt; are done, they then wait for the other threads to finish doing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MyFuncB()&lt;/code&gt;. This leads to extremely poor hardware utilization, as each thread will, at some point, have to wait and do nothing. This problem gets even worse if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MyFuncA()&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MyFuncB()&lt;/code&gt; are costly to evaluate, which increases the wait times. This is exactly the problem in path tracing: too many conditionals lead to wasted hardware utilization. Some paths may terminate early on, and be completely idle until long running paths terminate.&lt;/p&gt;

&lt;h1 id=&quot;improving-hardware-utilization&quot;&gt;Improving Hardware Utilization&lt;/h1&gt;

&lt;p&gt;In 2009, a ground-breaking research paper by researchers Timo Aila and Samuli Laine at Nvidia published in the &lt;em&gt;High Performance Graphics&lt;/em&gt; journal showed the results of experiments with various traversal algorithms, which included the performance of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;while-while&lt;/code&gt; traversal algorithm:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function traverse(root)
    node = root
    while ray is not terminated
        while node is not a leaf node
            move to the next node
        while node is a leaf node
            intersect with the primitives in node
            move to the next node
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At first glance, this code seems to be properly parallelized by ensuring that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;node&lt;/code&gt; is either a parent or leaf node at the beginning of the two inner &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;while&lt;/code&gt; loops. To test how well this algorithm is suited towards GPU ray tracing, a simulator was devised that “ran” the algorithm assuming perfect conditions in memory usage to set an upper bound for performance. The simulator also recorded the hardware utilization (referred to as “SIMD efficiency” in their paper) to gain further information. Surprisingly, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;while-while&lt;/code&gt; traversal only acheived approximately 55% efficiency, meaning that it was poorly parallelized. Upon further inspection, this becomes obvious: the inner &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;while&lt;/code&gt; loops have wildly varying execution times before they end. This means that, for example, while most rays had found a leaf node early on, some threads are stalling the entire warp by still going deeper into the tree. Then, when some nodes have exhausted the leaf nodes they have left to intersect, some threads are still going through each possible intersection. Then the loop was reorganized to follow an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;if-if&lt;/code&gt; structure:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function traverse(root)
    node = root
    while ray is not terminated
        if node is not a leaf
            proceed to the next node
        if node is a leaf
            intersect with the primitives in node
            move to the next node
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function executres a lot better. This is because instead of having loops with can have varying execution times, we only have two &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;if-if&lt;/code&gt; conditionals, which execute more or less in a constant amount of time . The traversal logic if the node is not a leaf may take a varying amount of time, and leaves can have a varying amount of primitives referenced with in them, so it is not perfectly constant. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;if-if&lt;/code&gt; was tested by the simulator and showed the perforamance skyrocketed to 75% efficiency.&lt;/p&gt;

&lt;h1 id=&quot;persistent-gpu-computing&quot;&gt;Persistent GPU Computing&lt;/h1&gt;

&lt;p&gt;In normal GPU compute applications, if you have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; tasks to process, you tell the GPU to dispatch &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; threads and each thread will work on a single task. However, in many cases, the number tasks exceeds the number of available threads, so a hardware scheduler is needed to dispatch tasks at optimial timings. An optimization GPU hardware schedulers often use is to pause a warp’s execution of its tasks and move onto another set of tasks (this strategy is commonly used when the entire warp is idling because it is waiting on memory reads to return). This works very well for regular workloads, but for highly irregular workloads like ray-tracing, the hardware scheduler becomes our enenmy. To bypass it, the &lt;em&gt;persistent threads&lt;/em&gt; programming model is used. Simply put, we launch just enough “tasks”” to completely fill our GPU, and implement our own software scheduler that gathers work for our threads. This has the benefit of not letting our hardware scheduler interrupt our program execution as it has no tasks to replace it with. Our software scheduler can be implemented by fetching our actual tasks from a work pool. One way we can implement persistent threads in ray tracing is to first represent our screen not as a 2D array but a long 1D array, and associate for each index in it a pixel coordinate on the 2D array. We can then increment atomic counters (until our 1D array is full) to get a pixel from this 1D array. An implementation of persistent threads in GLSL would look like this:&lt;/p&gt;

&lt;div class=&quot;language-glsl highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_size_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_size_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_size_z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// What this means is that we run one warp per each dispatch &lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batchPortion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// How many tasks our warp will process. This should be the number of threads in a warp&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batchSize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batchPortion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// On a GTX 285, 3 is the optimal multiplier [1], but 1 seems to perform better on my GTX 980&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;shared&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nextRay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;// In this warp, what will our next ray be?&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;shared&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rayCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// In this warp, how many rays are left? (if it is zero we need to get more tasks)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std430&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buffer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;globalNextRayBuf&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;uint&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;globalNextRay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Initially zero&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;threadIdx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;gl_LocalInvocationID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// Initialize all to batchPortion since we will subtract batchPortion later to make it zero&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threadIdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rayCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batchPortion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threadIdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;// Update our remaining tasks&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nextRay&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batchPortion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;rayCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batchPortion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;// If we are out of rays, then fetch some more&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rayCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;nextRay&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;atomicAdd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;globalNextRay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batchSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;rayCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batchSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;memoryBarrierShared&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Flush the value out of volatile memory&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// Find the ray we are processing for this specific thread&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;uint&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rayIndex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nextRay&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threadIdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rayIndex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;globalRayCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// Complete our ray task&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;ivec2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pixel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ConvertIndexToCoordiante&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rayIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; 
        &lt;span class=&quot;kt&quot;&gt;vec3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RayTrace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pixel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;imageStore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;screen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pixel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;vec4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The implemenation of persistent threads begins by grabbing 32 tasks per each warp (since the number of threads in a warp is 32) and then proceeds to go burn through the 32 threads before getting more. This allows us to bypass the hardware scheduler and have more performance. Much more. In fact, from my testing, I went from 13 FPS to outright 20 FPS in one scene. Simulation results show that this is close to 85% of maximum performance.&lt;/p&gt;

&lt;h1 id=&quot;persistent-regeneration&quot;&gt;Persistent Regeneration&lt;/h1&gt;

&lt;p&gt;In a path tracing loop, paths are often terminated because they either leave the scene or have too low energy. This is problematic for path tracers on the GPU because many threads stay idle because they were terminated early on while others continue long-running paths. To circumvent this issue, I choose to replace a path with a brand new one every time it was terminated by having too low energy. This gave me another 3-5 FPS boost.&lt;/p&gt;

&lt;h1 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;Timo Aila and Samuli Laine. 2009. Understanding the Efficiency of Ray Traversal on GPUs. In &lt;em&gt;Proceedings of the Conference on High Performance Graphics 2009&lt;/em&gt; (HPG ’09), Association for Computing Machinery, New Orleans, Louisiana, 145–149. DOI:https://doi.org/10.1145/1572769.1572792&amp;lt;&lt;/li&gt;
  &lt;li&gt;Kshitij Gupta, Jeff A. Stuart, and John D. Owens. 2012. A study of Persistent Threads style GPU programming for GPGPU workloads. In &lt;em&gt;2012 Innovative Parallel Computing&lt;/em&gt; (InPar), 1–14. DOI:https://doi.org/10.1109/InPar.2012.6339596&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Saad Amin</name></author><category term="gpu" /><summary type="html">In this blog post, I go over how we can design efficient GPU BVH traversal algorithms.</summary></entry></feed>